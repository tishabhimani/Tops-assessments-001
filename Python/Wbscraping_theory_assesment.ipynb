{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fb5bb65-2ce8-4c51-bec4-760b9167fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Any Website as per Your Faculty Suggest and the requests library for Webpage\n",
    "#1) Inspect the website's HTML source and identify the right URLs to download. \n",
    "#Answer\n",
    "#Inspect the website's HTML source and identify the right URLs\n",
    "\n",
    "#Open the website in a browser and right-click on the page. Select \"Inspect\" or \"Inspect Element\" to view the HTML structure.\n",
    "#Look for the target data (e.g., articles, product details, etc.), identify unique selectors (such as class or id), and extract URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4323dfc8-8b90-46f5-b8a8-84763343cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Download and save web pages locally using the requests library. \n",
    "#Answer\n",
    "import requests\n",
    "\n",
    "def download_page(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open('page.html', 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        print(f\"Page downloaded successfully from {url}\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page from {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc82bb83-9c93-41b8-b4ab-325440fddbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) Create a function to automate downloading for differenttopics/search queries. \n",
    "#Answer\n",
    "def download_search_page(base_url, search_query):\n",
    "    # Assuming the website supports query parameters\n",
    "    query_url = f\"{base_url}?search={search_query}\"\n",
    "    download_page(query_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d10dcb8-f527-4d41-98e3-b5a742cb5c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4) Use Beautiful Soup to parse and extract information \n",
    "#Answer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_page(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d565337f-2d9b-4e4c-9106-7099f7ebfc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5) Parse and explore the structure of downloaded web pages using beautiful soup. \n",
    "#Answer\n",
    "def explore_structure(soup):\n",
    "    # For example, print all links from the page\n",
    "    links = soup.find_all('a')\n",
    "    for link in links:\n",
    "        print(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ef1837a-1f0c-48a3-b8df-34e8e96c7786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6) Use the right properties and methods to extract the required information. \n",
    "#Answer\n",
    "def extract_information(soup):\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "\n",
    "    # Example: Extract article titles and descriptions\n",
    "    articles = soup.find_all('div', class_='article')\n",
    "    for article in articles:\n",
    "        title = article.find('h2').text.strip()\n",
    "        description = article.find('p').text.strip()\n",
    "        titles.append(title)\n",
    "        descriptions.append(description)\n",
    "\n",
    "    return titles, descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c0287ce-7fea-4065-ae9b-e49fe1003ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7) Create functions to extract from the page into lists and dictionaries. \n",
    "#Answer\n",
    "def extract_to_dict(soup):\n",
    "    data = []\n",
    "    articles = soup.find_all('div', class_='article')\n",
    "    for article in articles:\n",
    "        title = article.find('h2').text.strip()\n",
    "        description = article.find('p').text.strip()\n",
    "        data.append({'title': title, 'description': description})\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9255f09-e117-4d26-8616-bfc4744ed10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8) Create functions for the end-to-end process of downloading, parsing, and saving CSVs. \n",
    "#Answer\n",
    "import csv\n",
    "\n",
    "def save_to_csv(data, filename='output.csv'):\n",
    "    keys = data[0].keys() if data else []\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "def download_parse_and_save(base_url, search_query):\n",
    "    # Step 1: Download the page\n",
    "    download_search_page(base_url, search_query)\n",
    "\n",
    "    # Step 2: Parse the downloaded page\n",
    "    soup = parse_page('page.html')\n",
    "\n",
    "    # Step 3: Extract information\n",
    "    data = extract_to_dict(soup)\n",
    "\n",
    "    # Step 4: Save to CSV\n",
    "    save_to_csv(data)\n",
    "\n",
    "# Example usage\n",
    "base_url = 'https://example.com/search'\n",
    "search_query = 'data science'\n",
    "download_parse_and_save(base_url, search_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
